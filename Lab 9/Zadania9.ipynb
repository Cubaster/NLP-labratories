{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D48rgmQSB0B6"
   },
   "source": [
    "# Laboratoria 9: BERT i atencja\n",
    "\n",
    "\n",
    "### Zadanie 1 (3 pkt), atencja dekodera względem (en)kodera\n",
    "\n",
    "Poniżej znajdują się dwie macierze, `encoder_states` oraz `decoder_states` reprezentujące stan warstwy ukrytej po przetworzeniu każdego slowa z enkodera i dekodera. Pojedynczy stan warstwy ukrytej zawiera embedding o dlugosci = 3. W enkoderze mamy 4 stany warstwy ukrytej RNNów, gdyż przetwarzamy sekwencję 4 tokenów.\n",
    "\n",
    "W dekoderze mamy 5 tokenów, które powinny być wygenerowane z sekwencji przetwarzanej (en)koderem.\n",
    "\n",
    "Zadanie polega na:\n",
    "a) Obliczniu podobieństwa wszystkich embeddingów z dekodera (queries) względem wszystkich embeddingów kolejnych stanów (en)kodera (keys) [pamiętajcie, że macierze potrafią w transponowanie. W `NumPy` macierz transponujemy za pomocą `macierz.T`]\n",
    "\n",
    "b) Na utworzonej macierzy podobieństwa należy wykonać softmax (zaimportowany z scipy). Uwaga:  pamiętajcie, żeby aplikować softmax w dobrym wymiarze. Wszystkie stany ukryte enkodera powinny zostac zasoftmaksowane względem zadanego stanu dekodera, nie odwrotnie. W scipy, funkcja softmax zawiera argument axis, który może pomóc.\n",
    "\n",
    "c) Należy wykorzystać macierz atencji z kroku b) i `encoder_states` do wygenerowania macierzy zawierającej wektory kontekstu dla każdego tokenu z dekodera.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vQsum9iYATge"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a) Probabilities: \n",
      " [[  4.806   2.376   7.762   1.129]\n",
      " [ 12.164 -12.645  73.935   3.636]\n",
      " [ 27.79  -16.962  94.002   7.137]\n",
      " [ 18.702  -5.184  42.616   4.501]\n",
      " [ 64.38   49.86   56.21   14.45 ]]\n",
      "b) Softmax: \n",
      "[[4.91780633e-02 4.32948093e-03 9.45248312e-01 1.24414389e-03]\n",
      " [1.49003187e-27 2.50486173e-38 1.00000000e+00 2.94803216e-31]\n",
      " [1.75587568e-29 6.44090821e-49 1.00000000e+00 1.88369172e-38]\n",
      " [4.11416552e-11 1.74069934e-21 1.00000000e+00 2.79811669e-17]\n",
      " [9.99716568e-01 4.94220792e-07 2.82937800e-04 2.06801368e-22]]\n",
      "c) Context: \n",
      "[[ 9.69108631  0.35799187  0.59163688]\n",
      " [10.2         0.2         0.3       ]\n",
      " [10.2         0.2         0.3       ]\n",
      " [10.2         0.2         0.3       ]\n",
      " [ 1.20254471  3.39909302  5.59850122]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "# scipy.special.softmax(x, axis=None)\n",
    "\n",
    "encoder_states = np.array(\n",
    "    [[1.2, 3.4, 5.6],    # embedding z warstwy ukrytej enkodera w kroku 1,  np. dla slowa Ala\n",
    "    [-2.3, 0.2, 7.2],   # embedding z warstwy ukrytej enkodera w kroku 2,  np. dla slowa ma\n",
    "    [10.2, 0.2, 0.3],   # embedding z warstwy ukrytej enkodera w kroku 3,  np. dla slowa kota\n",
    "    [0.4, 0.7, 1.2]]    # embedding z warstwy ukrytej enkodera w kroku 4,  np. dla tokenu <EOS> (koniec sekwencji)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "decoder_states = np.array(\n",
    "    [[0.74, 0.23, 0.56],  # embedding z warstwy ukrytej dekodera w kroku 1,  np. przed wygenerowaniem slowa Alice\n",
    "    [7.23, 0.12, 0.55],  # embedding z warstwy ukrytej dekodera w kroku 2,  np. przed wygenerowaniem slowa owns\n",
    "    [9.12, 4.23, 0.44],  # embedding z warstwy ukrytej dekodera w kroku 3,  np. przed wygenerowaniem slowa a\n",
    "    [4.1, 3.23, 0.5],    # embedding z warstwy ukrytej dekodera w kroku 4,  np. przed wygenerowaniem slowa cat\n",
    "    [5.2, 3.1, 8.5]]     # embedding z warstwy ukrytej dekodera w kroku 5,  np. przed wygenerowaniem slowa cat\n",
    ")\n",
    "\n",
    "probabilities = decoder_states.dot(encoder_states.T)\n",
    "\n",
    "print(f\"a) Probabilities: \\n {probabilities}\")\n",
    "\n",
    "softmaxResult = softmax(probabilities, axis=1)\n",
    "print(f\"b) Softmax: \\n{softmaxResult}\")\n",
    "\n",
    "print(f\"c) Context: \\n{softmaxResult.dot(encoder_states)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwhDwdaTnPku"
   },
   "source": [
    "**Oczekiwane wartości:**\n",
    "\n",
    "a) \n",
    "[[  4.806   2.376   7.762   1.129]\n",
    " [ 12.164 -12.645  73.935   3.636]\n",
    " [ 27.79  -16.962  94.002   7.137]\n",
    " [ 18.702  -5.184  42.616   4.501]\n",
    " [ 64.38   49.86   56.21   14.45 ]] \n",
    "\n",
    "\n",
    "b) \n",
    "[[4.91780633e-02 4.32948093e-03 9.45248312e-01 1.24414389e-03]\n",
    " [1.49003187e-27 2.50486173e-38 1.00000000e+00 2.94803216e-31]\n",
    " [1.75587568e-29 6.44090821e-49 1.00000000e+00 1.88369172e-38]\n",
    " [4.11416552e-11 1.74069934e-21 1.00000000e+00 2.79811669e-17]\n",
    " [9.99716568e-01 4.94220792e-07 2.82937800e-04 2.06801368e-22]] \n",
    "\n",
    "c) \n",
    "[[ 9.69108631  0.35799187  0.59163688]\n",
    " [10.2         0.2         0.3       ]\n",
    " [10.2         0.2         0.3       ]\n",
    " [10.2         0.2         0.3       ]\n",
    " [ 1.20254471  3.39909302  5.59850122]]\n",
    " \n",
    " (albo to samo transponowane)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9220arNHo1V"
   },
   "source": [
    "## Zadanie 2 (2 punkty): tokenizacja tekstu \n",
    "\n",
    "Korzystając z biblioteki transformers (https://huggingface.co/transformers/) wczytaj tokenizator BERTa (BERT to już wytrenowany (pretrenowany) model, oparty o ideę transformera (a w zasadzie o jego enkoder)). Ponieważ model jest gotowy i można go wykorzystać do generowania embeddingów tokenów, ważnym jest, aby tokenizacja była przeprowadzona identycznie do tego jak podczas trenowania BERTa.\n",
    "\n",
    "Wybierzmy pretrenowany tokenizator o nazwie `bert-base-uncased` i zobaczmy jaki będzie efekt tokenizacji na tekście zawartym w zmiennej `text_to_tokenize`.\n",
    "\n",
    "Zwróć uwagę na to, że niektóre rzadkie słowa zostały podzielone na subtokeny -- zgodnie z algorytmem WordPiece jaki omawialiśmy na przedostatnim spotkaniu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yxcU6_pYNgfP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\anaconda\\lib\\site-packages (4.19.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in d:\\anaconda\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in d:\\anaconda\\lib\\site-packages (from transformers) (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\anaconda\\lib\\site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\anaconda\\lib\\site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\anaconda\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "# Uruchom mnie proszę\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uMLn-hE8L6Zh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens ids: \n",
      " [101, 1045, 1005, 2310, 4149, 1037, 2047, 14246, 2226, 2197, 2095, 2009, 2001, 16216, 14821, 19387, 2595, 24622, 2692, 102] \n",
      " and coresponding tokens: \n",
      " ['[CLS]', 'i', \"'\", 've', 'bought', 'a', 'new', 'gp', '##u', 'last', 'year', 'it', 'was', 'ge', '##force', 'rt', '##x', '306', '##0', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "text_to_tokenize = \"I've bought a new GPU last year it was GeForce RTX 3060\"\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokens = tokenizer(text_to_tokenize)\n",
    "print(f\"tokens ids: \\n {tokens['input_ids']} \\n and coresponding tokens: \\n {tokenizer.convert_ids_to_tokens(tokens['input_ids'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFwEeX2GOxEp"
   },
   "source": [
    "## Zadanie 3 (brak punktów):\n",
    "Poniżej znajduje się kod wykorzystujący przygotowane wcześniej zmienne `tokenizer` i `tokens` i które dla każdego tokenu z tokens generuje embedding. W odróżnieniu od GloVe, te embeddingi są świadome kontekstu w jakim właśnie występują. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XHlN4iH8P0Sv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 768])\n",
      "tensor([-2.4855e-01,  6.4821e-01, -4.4505e-01, -5.3101e-01, -3.7247e-01,\n",
      "        -4.5230e-01,  7.0275e-01, -3.2836e-01,  4.3371e-02, -2.0077e-01,\n",
      "         7.7430e-02,  1.2603e-02, -5.3415e-03,  2.3520e-02,  1.0070e+00,\n",
      "        -2.1925e-01,  7.9879e-01, -6.1398e-01,  2.2324e-01, -1.2752e-01,\n",
      "        -5.6451e-01, -2.5957e-01, -5.0997e-03,  6.4843e-01, -1.8477e-02,\n",
      "        -5.4585e-01,  3.8544e-01, -8.5510e-01, -2.4844e-02, -2.6924e-01,\n",
      "         1.7788e+00, -1.1623e+00, -5.5582e-01,  6.0392e-01, -3.9546e-01,\n",
      "        -1.1233e+00,  5.5031e-01, -2.4410e-01,  1.5945e-01,  1.7484e-01,\n",
      "        -9.7499e-01, -6.1897e-01,  1.7481e-01,  4.0425e-01, -7.5808e-01,\n",
      "        -1.3810e+00,  2.1792e-01,  3.6299e-01,  5.7498e-01,  5.1727e-01,\n",
      "         1.2242e-02,  2.7538e-01, -1.9612e-02,  4.8239e-01, -9.6976e-02,\n",
      "         6.5601e-01, -1.9422e-01, -8.0068e-01, -5.9801e-01, -7.7547e-01,\n",
      "         1.2948e+00, -8.7721e-02, -4.9269e-01, -2.1871e-01,  3.5231e-01,\n",
      "         2.9646e-01, -2.7637e-01,  6.6774e-01, -1.2779e-01,  2.1254e-01,\n",
      "        -4.7112e-01,  1.3219e+00, -7.8359e-01, -5.0490e-01, -1.9864e-02,\n",
      "        -2.3320e-01, -3.5068e-01,  7.3894e-01,  4.6744e-01, -8.9916e-01,\n",
      "        -2.9609e-01,  7.3709e-01, -1.0722e+00,  9.1281e-01, -4.3644e-01,\n",
      "        -1.1438e-01,  1.6309e+00, -5.6392e-01, -2.2045e-01,  9.7167e-02,\n",
      "        -5.4177e-02, -6.6374e-01,  3.3455e-01, -2.1237e-01,  6.2790e-01,\n",
      "         8.5682e-01, -5.2601e-01,  7.4402e-01,  2.1455e-02, -2.4233e-02,\n",
      "        -1.4052e-01,  1.8242e-01,  1.4092e-01,  2.5126e-01, -9.8085e-01,\n",
      "        -4.7977e-01,  2.2112e-01,  2.8406e-01, -6.5492e-01,  9.4223e-01,\n",
      "        -4.2531e-01, -5.9052e-01, -1.2971e+00, -9.0449e-01, -9.2000e-03,\n",
      "         5.1308e-01, -1.1152e-01,  5.0188e-01,  4.4554e-01,  3.1383e-01,\n",
      "         4.7023e-01,  1.5259e-01,  3.9696e-01,  7.9677e-01,  7.3824e-02,\n",
      "        -1.8629e-01, -2.9774e-01,  3.2294e-01, -4.7870e-01,  5.4245e-02,\n",
      "        -2.5933e-01,  6.4299e-01, -8.6245e-01,  2.5321e-01, -1.8863e-01,\n",
      "        -6.2540e-01,  5.7324e-01,  2.5809e-01, -1.0573e+00, -4.9666e-01,\n",
      "        -4.0621e-01, -9.2945e-02,  2.9902e-01, -5.0318e-01, -5.3838e-01,\n",
      "        -1.5972e-01, -2.6075e-01, -7.7966e-01, -5.5628e-01, -6.4862e-02,\n",
      "         4.1449e-02, -5.0314e-01,  1.5495e+00, -3.6189e-01,  4.0437e-02,\n",
      "        -5.2628e-01, -5.7194e-01, -2.4582e-01, -1.0430e+00, -7.8574e-01,\n",
      "         6.6235e-01,  2.0284e-01,  1.2442e-02,  1.9109e-01,  3.2785e-01,\n",
      "         6.7220e-01,  3.1678e-01,  1.8997e-01, -9.0745e-01, -2.0850e-01,\n",
      "         2.8443e-01, -6.1362e-01, -1.6194e-01, -1.3602e-01, -1.7316e-01,\n",
      "         7.5293e-01,  7.8956e-01, -2.3445e-01, -2.6958e-01,  4.7634e-01,\n",
      "        -5.7509e-01,  8.0286e-01, -3.7699e-01, -3.6351e-01,  9.6723e-01,\n",
      "         4.9504e-01, -1.3504e-01,  2.4954e-01,  8.3884e-01,  3.2310e-01,\n",
      "        -7.5321e-01,  1.2041e-01, -9.6307e-02,  2.3805e-01,  6.6209e-01,\n",
      "        -2.7415e-01,  3.1663e-01,  2.5078e-01, -4.8513e-01,  1.4742e-01,\n",
      "        -3.3288e-01, -1.6896e-01, -3.5077e-01,  2.1362e-01, -1.0920e+00,\n",
      "        -7.6415e-01,  5.1040e-01,  2.3048e-01,  3.8046e-01, -8.0899e-01,\n",
      "         1.5033e-01,  6.0375e-01,  1.0951e-01,  2.4340e-01,  8.7622e-01,\n",
      "         1.8856e-01, -3.4384e-01, -6.9223e-01, -1.6903e-01, -2.9668e-02,\n",
      "        -3.8846e-01, -8.1606e-01, -7.3828e-01,  7.0545e-01,  4.4207e-01,\n",
      "         1.3635e-01,  6.1434e-01, -1.3168e+00,  1.5072e-01,  2.0195e-01,\n",
      "        -3.7086e-02, -1.1600e+00,  9.5462e-01,  7.1294e-01, -4.8293e-01,\n",
      "        -3.4892e-01, -9.5731e-02, -8.0471e-02,  2.2264e-01, -4.7594e-01,\n",
      "         6.4236e-01,  1.2453e+00, -1.2352e-01, -8.3352e-01,  2.8359e-01,\n",
      "         5.6134e-01, -2.6840e-01, -4.5411e-01, -8.4799e-01, -5.8606e-01,\n",
      "        -2.4987e-01,  1.1627e-01,  5.6522e-02, -4.2828e-02, -4.0870e-01,\n",
      "        -1.1874e+00,  4.4330e-01,  3.4774e-01, -5.5283e-01, -1.9855e-01,\n",
      "         3.3271e-01,  1.9230e-01,  1.2372e-01,  3.8279e-01,  1.2217e-01,\n",
      "         2.9158e-01, -5.4700e-01,  1.0658e+00,  6.1423e-01,  6.8804e-01,\n",
      "        -1.7036e-01,  5.9557e-01, -7.5619e-01,  4.3745e-01, -4.3301e-01,\n",
      "         4.4853e-01,  2.8714e-01, -4.2169e-01,  2.9995e-01,  3.1578e-01,\n",
      "         4.1756e-01,  2.7854e-01,  4.6558e-01, -1.9473e-01,  1.1463e-01,\n",
      "         3.7208e-01, -2.8867e-01, -1.8366e-01,  1.9124e-01, -3.4057e-01,\n",
      "         2.6599e-01,  4.3035e-01, -9.3394e-01,  1.3037e-01,  2.3737e-01,\n",
      "         1.8492e-01, -1.6802e-01,  8.9311e-01,  4.1679e-01,  4.6561e-01,\n",
      "        -1.2303e-01, -3.7387e-01,  6.7693e-01, -1.9507e-01,  2.2857e-01,\n",
      "        -4.8632e-01, -1.4380e-01,  4.1609e-01, -3.5559e+00,  1.0986e+00,\n",
      "         6.2660e-01, -6.2327e-02,  8.2399e-02, -5.9821e-01,  1.9209e-01,\n",
      "         1.2397e-02, -4.3049e-01,  2.5148e-01, -7.4299e-01,  6.5441e-01,\n",
      "         2.5569e-03, -1.3048e-01,  1.3415e-01,  1.0279e+00,  6.4064e-02,\n",
      "         1.0281e-02, -6.7075e-01,  1.8755e-01,  7.1755e-03, -2.4486e-01,\n",
      "        -6.2726e-01,  1.7102e-01,  6.6649e-01, -3.3725e-02, -4.3737e-01,\n",
      "        -4.6982e-01, -2.8370e-01, -3.5400e-01,  2.0996e-01, -1.7779e-01,\n",
      "         2.4855e-01,  7.2802e-01,  4.8932e-01, -2.9279e-01, -6.3857e-01,\n",
      "        -4.1220e-01,  2.2721e-01, -8.3957e-01, -5.6522e-01, -1.7862e+00,\n",
      "        -2.4333e-01, -6.4559e-01,  4.5112e-01, -4.3180e-01,  1.4027e-01,\n",
      "        -8.5483e-02,  9.9935e-02,  2.7667e-01,  9.6359e-01, -4.1384e-01,\n",
      "        -1.5362e-01,  1.9000e-01,  1.9585e-01, -7.7532e-02,  8.8642e-01,\n",
      "         8.7736e-01, -8.2530e-01, -1.2210e+00, -1.0895e-01,  1.0331e-01,\n",
      "        -2.3881e+00, -1.0825e-02,  4.5416e-01, -1.3064e+00, -5.6483e-01,\n",
      "        -7.0093e-01, -4.4890e-01,  2.6482e-01, -5.7273e-01,  1.0655e+00,\n",
      "         3.0407e-01, -1.5240e-01,  3.5444e-03,  6.9588e-01, -1.9933e-02,\n",
      "         4.6319e-03, -5.7890e-01, -7.8957e-01, -1.5433e-01, -2.0839e-01,\n",
      "         3.2785e-01,  3.8013e-02, -7.1902e-01,  7.3102e-01, -2.4995e-01,\n",
      "        -3.1801e-01, -5.7997e-01, -4.5433e-01,  1.1307e+00,  2.5195e-01,\n",
      "         1.4327e-01, -4.0222e-01,  6.3066e-01,  4.6442e-01,  6.3555e-01,\n",
      "        -5.8952e-01, -8.7842e-02,  8.6420e-03,  6.9671e-01,  6.3694e-01,\n",
      "        -2.1387e-01,  1.3470e-01, -1.1221e-02, -9.9962e-02, -9.3165e-01,\n",
      "         3.6331e-01,  9.4315e-01,  2.1494e-01, -2.3910e-02, -4.5226e-01,\n",
      "         9.4149e-02, -2.7822e-01, -1.2286e-01, -1.4523e-01,  5.5581e-01,\n",
      "         4.4973e-01,  9.7113e-01, -8.6990e-01, -3.5979e-01,  8.9532e-01,\n",
      "        -1.3081e+00, -4.7362e-01, -9.0184e-01,  1.1307e+00, -5.2907e-01,\n",
      "         1.2889e-01,  1.3262e-01,  4.9297e-01,  1.8087e-01, -3.7575e-01,\n",
      "        -5.4401e-01,  4.6241e-01, -1.5203e-01,  3.1687e-01,  4.8990e-01,\n",
      "        -5.2032e-01, -3.6158e-01, -5.1684e-01, -2.4571e-01,  4.0409e-02,\n",
      "         4.1860e-01, -2.2870e-01, -1.6600e-01,  1.7082e-01, -5.6892e-01,\n",
      "         7.2237e-02, -2.1909e-01,  1.0136e-01, -5.4183e-01, -7.0899e-01,\n",
      "        -1.8742e-01, -3.5751e-02,  6.1466e-02, -1.2098e-01, -4.8068e-01,\n",
      "        -4.7240e-01,  3.9565e-01, -1.2750e+00,  1.2115e+00, -5.9105e-01,\n",
      "         1.4736e-01,  6.7185e-01, -5.0643e-01,  1.4196e+00,  1.2948e-01,\n",
      "         6.7752e-01,  2.5796e-01, -2.9861e-01,  6.2824e-01, -6.5813e-01,\n",
      "        -1.7188e-01, -1.1978e+00, -9.3099e-02,  1.3482e+00,  6.0527e-02,\n",
      "         3.8353e-01, -6.4087e-01, -3.0751e-01,  5.3329e-01,  4.4622e-01,\n",
      "         7.7318e-01, -6.7251e-02,  2.1205e-01, -3.1703e-01,  2.0134e-01,\n",
      "        -9.5688e-01,  2.9477e-02,  6.7284e-01,  4.6805e-01,  4.5667e-01,\n",
      "         2.1908e-01, -8.5335e-02, -4.0770e-01, -7.1971e-01, -6.7793e-02,\n",
      "        -1.3972e-01,  1.0904e-02,  9.5069e-02,  3.6509e-01, -2.9236e-01,\n",
      "         2.1434e-01, -2.7148e-01, -2.9394e-01,  3.3060e-01, -4.1964e-01,\n",
      "        -1.1958e-02,  3.6661e-01, -1.1196e-03, -5.7029e-03, -2.3401e-01,\n",
      "        -4.9093e-01, -3.5427e-01, -6.4338e-01, -2.4370e-01, -6.3230e-02,\n",
      "         4.7673e-01, -5.9875e-01,  2.1349e-02,  7.4106e-01,  2.1441e-01,\n",
      "        -3.3235e-01, -4.9721e-01,  2.7316e-01, -2.6595e-01, -8.2350e-01,\n",
      "        -1.3447e-01, -2.6621e-01, -9.9975e-01, -3.6053e-01, -1.2084e+00,\n",
      "         5.3119e-02,  3.2141e-01, -7.2036e-01, -7.5120e-02,  5.1660e-02,\n",
      "         1.6494e-01,  1.1249e+00,  2.2725e-01,  4.1656e-01,  8.1648e-01,\n",
      "        -4.0551e-01,  7.2856e-01,  9.2206e-01,  4.0558e-01, -1.1946e+00,\n",
      "        -2.2992e-01,  6.0654e-01, -2.5034e-03, -6.8292e-02, -7.1522e-02,\n",
      "         1.4490e-01, -2.1059e-01, -1.2625e-01, -3.1588e-02, -5.0977e-02,\n",
      "         7.8895e-01, -7.3743e-01,  1.1877e-01,  1.8830e-01,  6.9879e-01,\n",
      "        -6.4266e-02,  1.8523e-02,  3.8645e-01, -7.4206e-02,  8.4601e-02,\n",
      "         8.4816e-01, -9.3545e-01, -6.5483e-02, -1.1793e+00, -2.1913e-01,\n",
      "        -1.1471e-01,  4.2145e-01, -5.7279e-02, -8.2611e-02, -3.7731e-01,\n",
      "         1.3179e-01,  1.8892e-02,  7.8926e-02, -6.0809e-02, -2.9614e-01,\n",
      "         5.6265e-01,  1.2786e+00,  7.3489e-01, -1.3629e-01,  3.7862e-01,\n",
      "         7.3811e-01,  4.4368e-01, -3.7156e-01,  2.7988e-01,  3.8676e-02,\n",
      "        -3.2287e-01, -6.8871e-01, -3.5283e-02,  2.2005e-01,  5.5537e-01,\n",
      "         3.1325e-01, -7.2754e-01,  9.8483e-02,  4.3311e-01, -8.3360e-01,\n",
      "         3.1680e-01,  1.2840e+00,  9.7150e-04, -2.6515e-01,  4.6332e-01,\n",
      "        -9.1274e-01, -7.6127e-01, -4.6080e-01, -2.5767e-01,  4.4772e-01,\n",
      "         5.7990e-01, -9.9933e-02, -5.7029e-02, -8.3284e-01, -7.6744e-01,\n",
      "         7.2851e-01,  7.3822e-01, -4.7770e-02,  7.5718e-02, -6.3198e-01,\n",
      "         8.1606e-01,  4.0680e-03,  5.2449e-01, -7.5436e-01,  4.1136e-01,\n",
      "        -3.2356e-01, -2.3355e-01,  1.1334e+00, -2.8034e-01,  2.7624e-01,\n",
      "         1.0195e+00,  1.0660e-01,  1.3986e-01, -1.4343e-01, -1.3243e+00,\n",
      "         2.1105e-01,  7.4091e-01,  5.1908e-02, -2.7890e-01,  7.9027e-01,\n",
      "         1.1033e+00, -1.0771e-01,  1.0207e-01,  2.4599e-01,  6.7522e-01,\n",
      "        -1.0718e+00,  2.3707e-01, -2.8616e-01,  6.4630e-01,  2.8769e-01,\n",
      "        -4.3704e-01,  6.0091e-02, -1.3296e-01,  7.5951e-01,  1.2585e-01,\n",
      "         1.0465e+00,  5.3660e-01,  4.8555e-01, -4.5609e-01,  3.4909e-01,\n",
      "         9.4601e-02, -1.8373e-01, -1.4194e-01, -9.8919e-02, -2.0151e-01,\n",
      "        -1.1217e-01, -6.3937e-01,  1.7219e-01, -1.6661e-01, -6.0209e-02,\n",
      "        -2.8295e-01, -1.1094e+00,  5.0497e-01,  7.3545e-01, -6.0788e-01,\n",
      "        -2.5610e-01,  3.1812e-01, -2.8559e-01, -5.0417e-01, -1.7046e-01,\n",
      "         5.8475e-01, -5.8679e-01, -8.2155e-01, -2.4204e-02, -7.4340e-01,\n",
      "        -1.9414e-01, -4.0470e-01, -1.1973e+00,  7.3738e-01,  2.0456e-01,\n",
      "         7.6853e-01,  1.4206e-01, -6.5274e-01, -2.2269e-01,  5.7212e-01,\n",
      "        -4.4042e-02,  7.8591e-01,  5.1171e-01, -2.1875e-01, -1.1297e-01,\n",
      "        -1.2688e+00,  2.0703e-01,  1.2729e-01,  8.7465e-02,  7.9539e-01,\n",
      "         6.9996e-01, -5.5370e-01, -2.4057e-01,  5.4459e-01, -4.5611e-01,\n",
      "         1.3922e-01,  8.5277e-01,  8.0727e-03,  7.6058e-02,  9.0845e-01,\n",
      "         3.4145e-01, -1.1726e+00, -1.0227e-01, -3.4804e-01, -9.4293e-01,\n",
      "         1.0302e-01,  2.3639e-01, -9.2975e-02, -3.0444e-01,  1.3617e-01,\n",
      "        -2.8635e-01,  2.5191e-01,  1.4343e-01, -4.6722e-01, -9.8522e-02,\n",
      "        -3.1689e-01,  3.4934e-01, -1.5721e-01, -3.8967e-01,  3.0915e-01,\n",
      "         1.0259e+00,  2.4808e-01,  6.3048e-01,  5.2011e-01,  9.1367e-01,\n",
      "        -1.6353e-01,  6.7312e-01,  1.1136e+00,  4.4042e-01,  1.4400e-01,\n",
      "         1.2421e-01, -9.1991e-01, -7.7466e-01, -2.4124e-02, -6.2341e-01,\n",
      "         4.9523e-01, -3.7154e-01, -2.1687e-01, -4.7444e-01, -8.0754e-01,\n",
      "         4.3895e-02,  6.1856e-01,  2.6309e-01])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "import torch\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)  \n",
    "model.eval()  # nie chcemy trenowac modelu, tylko go wykorzystac\n",
    "\n",
    "tokens_with_specials = tokens['input_ids']  # BERT wymaga specjalnych tokenów [CLS] na poczatku i [SEP] separaującego pary zdań (BERT jest trenowany parami zdań)\n",
    "tokens_with_specials = tokenizer.convert_tokens_to_ids(tokens_with_specials)  # zamiana listy tokenow na listę identyfikatorów (liczb) ze slownika\n",
    "tokens_tensor = torch.tensor([tokens_with_specials])  # zamiana na tensor, opakowanie w batch\n",
    "\n",
    "segments = torch.tensor([[1] * len(tokens_with_specials)])  # wygeneruj maskę mówiącą o tym które tokeny nalezą do zdania 1, a ktore do 2. W naszym zadaniu wszystkie tokeny naleza do zdania 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segments)  # wygenerujmy embeddingi BERTem\n",
    "    tokens_embeddings = outputs['last_hidden_state'][0]  # wez pierwszy batch danych i ostatnią warstwę\n",
    "    print(tokens_embeddings.shape)  # 20x768, mamy 20 (sub)tokenów, (18 wlasciwych + cls + sep) i kazdy mapowany jest na wektor liczb o dlugosci 768\n",
    "    print(tokens_embeddings[1])  # wez embedding pierwszego subtokenu z sekwencji (przeskakujemy CLS token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWi18mj6T_S5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Zadania10-2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
